{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8beda1f",
   "metadata": {},
   "source": [
    "# ü™ô Detector de Monedas - Inspector de Calidad Casero\n",
    "\n",
    "Este notebook implementa el proyecto completo:\n",
    "1. **Preparaci√≥n del dataset** (formato COCO desde CVAT)\n",
    "2. **Fine-tuning de DETR** con Hugging Face\n",
    "3. **Evaluaci√≥n del modelo**\n",
    "4. **Exportaci√≥n a ONNX**\n",
    "\n",
    "‚ö†Ô∏è **Recomendaci√≥n**: Ejecutar en Google Colab con GPU habilitada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b171e",
   "metadata": {},
   "source": [
    "## üì¶ 1. Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1580ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "# Descomentar si est√°s en Google Colab\n",
    "\n",
    "# !pip install -q transformers datasets torch torchvision\n",
    "# !pip install -q pycocotools albumentations\n",
    "# !pip install -q onnx onnxruntime\n",
    "# !pip install -q timm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# PyTorch Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29dc77",
   "metadata": {},
   "source": [
    "## üìÅ 2. Configuraci√≥n del Dataset\n",
    "\n",
    "### Estructura esperada de carpetas:\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_001.jpg\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ annotations.json  (formato COCO exportado de CVAT)\n",
    "‚îî‚îÄ‚îÄ val/\n",
    "    ‚îú‚îÄ‚îÄ images/\n",
    "    ‚îî‚îÄ‚îÄ annotations.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üîß CONFIGURACI√ìN - MODIFICAR SEG√öN TU CASO\n",
    "# ============================================\n",
    "\n",
    "# Rutas al dataset (cambiar seg√∫n tu estructura)\n",
    "DATASET_PATH = \"./dataset\"  # Carpeta ra√≠z del dataset\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"val\")\n",
    "\n",
    "# Clases del proyecto\n",
    "CLASSES = [\"cara\", \"cruz\"]  # Tus etiquetas de CVAT\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Hiperpar√°metros de entrenamiento\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Modelo base\n",
    "MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "\n",
    "print(f\"Clases: {CLASSES}\")\n",
    "print(f\"N√∫mero de clases: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e42d05",
   "metadata": {},
   "source": [
    "## üìä 3. Cargar y Explorar el Dataset COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_annotations(annotation_path):\n",
    "    \"\"\"Carga las anotaciones COCO exportadas de CVAT.\"\"\"\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Estad√≠sticas del dataset:\")\n",
    "    print(f\"   - Im√°genes: {len(coco_data['images'])}\")\n",
    "    print(f\"   - Anotaciones: {len(coco_data['annotations'])}\")\n",
    "    print(f\"   - Categor√≠as: {[cat['name'] for cat in coco_data['categories']]}\")\n",
    "    \n",
    "    return coco_data\n",
    "\n",
    "# Cargar anotaciones (descomentar cuando tengas el dataset)\n",
    "# train_coco = load_coco_annotations(os.path.join(TRAIN_PATH, \"annotations.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(coco_data, images_path, idx=0):\n",
    "    \"\"\"Visualiza una imagen con sus anotaciones.\"\"\"\n",
    "    # Obtener imagen\n",
    "    img_info = coco_data['images'][idx]\n",
    "    img_path = os.path.join(images_path, img_info['file_name'])\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Obtener anotaciones de esta imagen\n",
    "    img_id = img_info['id']\n",
    "    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == img_id]\n",
    "    \n",
    "    # Crear mapeo de categor√≠as\n",
    "    cat_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    colors = {'cara': 'green', 'cruz': 'red'}\n",
    "    \n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']  # [x, y, width, height] en formato COCO\n",
    "        cat_name = cat_map[ann['category_id']]\n",
    "        color = colors.get(cat_name, 'blue')\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(bbox[0], bbox[1] - 5, cat_name, color=color, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f\"Imagen: {img_info['file_name']}\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar ejemplo (descomentar cuando tengas datos)\n",
    "# visualize_sample(train_coco, os.path.join(TRAIN_PATH, \"images\"), idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448fc01",
   "metadata": {},
   "source": [
    "## üîÑ 4. Dataset de PyTorch para DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para cargar datos COCO y prepararlos para DETR.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_path, annotation_path, processor):\n",
    "        self.images_path = images_path\n",
    "        self.processor = processor\n",
    "        \n",
    "        # Cargar anotaciones COCO\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        self.images = self.coco_data['images']\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "        \n",
    "        # Crear √≠ndice de anotaciones por imagen\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.images_path, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Obtener anotaciones\n",
    "        img_id = img_info['id']\n",
    "        annotations = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        # Preparar target en formato DETR\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            # COCO format: [x, y, width, height] -> convertir a [x_center, y_center, w, h] normalizado\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Normalizar por dimensiones de la imagen\n",
    "            img_w, img_h = image.size\n",
    "            x_center = (x + w / 2) / img_w\n",
    "            y_center = (y + h / 2) / img_h\n",
    "            w_norm = w / img_w\n",
    "            h_norm = h / img_h\n",
    "            \n",
    "            boxes.append([x_center, y_center, w_norm, h_norm])\n",
    "            labels.append(ann['category_id'] - 1)  # DETR espera labels desde 0\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "        \n",
    "        # Procesar con el processor de DETR\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"boxes\": target[\"boxes\"].tolist(), \"class_labels\": target[\"class_labels\"].tolist()},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Quitar dimensi√≥n batch\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
    "        labels = encoding[\"labels\"][0]\n",
    "        \n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24389a5e",
   "metadata": {},
   "source": [
    "## üß† 5. Cargar Modelo DETR Pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be42ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar processor y modelo\n",
    "processor = DetrImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Cargar modelo con n√∫mero de clases personalizado\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True  # Importante para cambiar el n√∫mero de clases\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado: {MODEL_NAME}\")\n",
    "print(f\"   - N√∫mero de par√°metros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   - Par√°metros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03743ee9",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 6. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df41c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Funci√≥n para agrupar muestras en un batch.\"\"\"\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Crear datasets (descomentar cuando tengas los datos)\n",
    "# train_dataset = CocoDetectionDataset(\n",
    "#     images_path=os.path.join(TRAIN_PATH, \"images\"),\n",
    "#     annotation_path=os.path.join(TRAIN_PATH, \"annotations.json\"),\n",
    "#     processor=processor\n",
    "# )\n",
    "# \n",
    "# val_dataset = CocoDetectionDataset(\n",
    "#     images_path=os.path.join(VAL_PATH, \"images\"),\n",
    "#     annotation_path=os.path.join(VAL_PATH, \"annotations.json\"),\n",
    "#     processor=processor\n",
    "# )\n",
    "# \n",
    "# print(f\"Train samples: {len(train_dataset)}\")\n",
    "# print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./detr_monedas_output\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision si hay GPU\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n de entrenamiento lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635853eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Trainer (descomentar cuando tengas datasets)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     data_collator=collate_fn,\n",
    "# )\n",
    "\n",
    "# Entrenar\n",
    "# print(\"üöÄ Iniciando entrenamiento...\")\n",
    "# trainer.train()\n",
    "# print(\"‚úÖ Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d18a8",
   "metadata": {},
   "source": [
    "## üîç 7. Inferencia - Probar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb42369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, processor, image_path, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Realiza predicci√≥n en una imagen y visualiza los resultados.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo DETR entrenado\n",
    "        processor: DetrImageProcessor\n",
    "        image_path: Ruta a la imagen\n",
    "        threshold: Umbral de confianza para mostrar predicciones\n",
    "    \"\"\"\n",
    "    # Cargar imagen\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocesar\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Mover a GPU si est√° disponible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Inferencia\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-procesar resultados\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, \n",
    "        target_sizes=target_sizes,\n",
    "        threshold=threshold\n",
    "    )[0]\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    colors = {0: 'green', 1: 'red'}  # cara=0, cruz=1\n",
    "    \n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = box.cpu().numpy()\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        label_idx = label.item()\n",
    "        label_name = CLASSES[label_idx]\n",
    "        confidence = score.item()\n",
    "        color = colors.get(label_idx, 'blue')\n",
    "        \n",
    "        # Dibujar caja\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            linewidth=3, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Etiqueta con confianza\n",
    "        ax.text(\n",
    "            x1, y1 - 10, \n",
    "            f\"{label_name}: {confidence:.2f}\", \n",
    "            color='white', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor=color, alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f\"Predicciones (umbral={threshold})\")\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"\\nüìä Detecciones encontradas: {len(results['scores'])}\")\n",
    "    for i, (score, label) in enumerate(zip(results[\"scores\"], results[\"labels\"])):\n",
    "        print(f\"   {i+1}. {CLASSES[label.item()]}: {score.item():.2%}\")\n",
    "\n",
    "# Ejemplo de uso (descomentar con una imagen real)\n",
    "# predict_and_visualize(model, processor, \"./test_image.jpg\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1d0f1",
   "metadata": {},
   "source": [
    "## üì¶ 8. Exportar a ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716aa635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, output_path=\"detector_monedas.onnx\", input_size=(800, 800)):\n",
    "    \"\"\"\n",
    "    Exporta el modelo DETR a formato ONNX.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo DETR entrenado\n",
    "        output_path: Ruta de salida del archivo .onnx\n",
    "        input_size: Tama√±o de entrada (alto, ancho)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparando modelo para exportaci√≥n ONNX...\")\n",
    "    \n",
    "    # Poner modelo en modo evaluaci√≥n y CPU\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    # Crear entrada dummy\n",
    "    dummy_input = torch.randn(1, 3, input_size[0], input_size[1])\n",
    "    \n",
    "    print(f\"   - Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Exportar\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        opset_version=12,\n",
    "        input_names=[\"pixel_values\"],\n",
    "        output_names=[\"logits\", \"pred_boxes\"],\n",
    "        dynamic_axes={\n",
    "            \"pixel_values\": {0: \"batch_size\"},\n",
    "            \"logits\": {0: \"batch_size\"},\n",
    "            \"pred_boxes\": {0: \"batch_size\"}\n",
    "        },\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    \n",
    "    # Verificar tama√±o del archivo\n",
    "    file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Modelo exportado exitosamente!\")\n",
    "    print(f\"   - Archivo: {output_path}\")\n",
    "    print(f\"   - Tama√±o: {file_size:.2f} MB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Exportar (descomentar cuando tengas el modelo entrenado)\n",
    "# onnx_path = export_to_onnx(model, \"detector_monedas.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_onnx_model(onnx_path):\n",
    "    \"\"\"\n",
    "    Verifica que el modelo ONNX es v√°lido.\n",
    "    \"\"\"\n",
    "    import onnx\n",
    "    \n",
    "    print(f\"üîç Verificando modelo ONNX: {onnx_path}\")\n",
    "    \n",
    "    # Cargar y verificar\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    \n",
    "    # Imprimir informaci√≥n\n",
    "    print(f\"\\n‚úÖ Modelo ONNX v√°lido!\")\n",
    "    print(f\"\\nüìä Informaci√≥n del modelo:\")\n",
    "    print(f\"   - IR Version: {onnx_model.ir_version}\")\n",
    "    print(f\"   - Opset Version: {onnx_model.opset_import[0].version}\")\n",
    "    \n",
    "    # Inputs\n",
    "    print(f\"\\n   Inputs:\")\n",
    "    for inp in onnx_model.graph.input:\n",
    "        print(f\"      - {inp.name}: {[dim.dim_value for dim in inp.type.tensor_type.shape.dim]}\")\n",
    "    \n",
    "    # Outputs\n",
    "    print(f\"\\n   Outputs:\")\n",
    "    for out in onnx_model.graph.output:\n",
    "        print(f\"      - {out.name}\")\n",
    "\n",
    "# Verificar (descomentar despu√©s de exportar)\n",
    "# verify_onnx_model(\"detector_monedas.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6687fcf",
   "metadata": {},
   "source": [
    "## ‚ö° 9. Inferencia con ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0044a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_onnx(onnx_path, image_path, processor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Realiza inferencia usando ONNX Runtime (m√°s r√°pido que PyTorch).\n",
    "    \"\"\"\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    print(f\"üîÑ Cargando modelo ONNX...\")\n",
    "    \n",
    "    # Crear sesi√≥n de inferencia\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Cargar y preprocesar imagen\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"np\")\n",
    "    \n",
    "    # Inferencia\n",
    "    print(f\"üöÄ Ejecutando inferencia...\")\n",
    "    outputs = session.run(\n",
    "        None,\n",
    "        {\"pixel_values\": inputs[\"pixel_values\"]}\n",
    "    )\n",
    "    \n",
    "    logits, pred_boxes = outputs\n",
    "    \n",
    "    # Post-procesar\n",
    "    probas = torch.softmax(torch.tensor(logits), dim=-1)[0, :, :-1]  # Excluir clase \"no object\"\n",
    "    keep = probas.max(-1).values > threshold\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inferencia completada!\")\n",
    "    print(f\"   - Detecciones sobre umbral {threshold}: {keep.sum().item()}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Ejemplo (descomentar cuando tengas el modelo)\n",
    "# inference_with_onnx(\"detector_monedas.onnx\", \"./test_image.jpg\", processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfda94",
   "metadata": {},
   "source": [
    "## üìã 10. Checklist del Proyecto\n",
    "\n",
    "### S√°bado - Datos ‚úÖ\n",
    "- [ ] Capturar 30-40 fotos de monedas\n",
    "- [ ] Crear cuenta en CVAT.ai\n",
    "- [ ] Subir im√°genes y crear Task\n",
    "- [ ] Definir labels: `cara`, `cruz`\n",
    "- [ ] Anotar todas las im√°genes con bounding boxes\n",
    "- [ ] Exportar en formato COCO 1.0\n",
    "- [ ] Dividir en train (80%) y val (20%)\n",
    "\n",
    "### Domingo - Modelo ‚úÖ\n",
    "- [ ] Subir notebook a Google Colab\n",
    "- [ ] Subir dataset a Colab\n",
    "- [ ] Ejecutar entrenamiento (20-50 √©pocas)\n",
    "- [ ] Evaluar en im√°genes de validaci√≥n\n",
    "- [ ] Exportar a ONNX\n",
    "- [ ] Verificar modelo ONNX\n",
    "- [ ] Probar inferencia con ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902837c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado (descomentar cuando entrenes)\n",
    "# model.save_pretrained(\"./detr_monedas_final\")\n",
    "# processor.save_pretrained(\"./detr_monedas_final\")\n",
    "# print(\"‚úÖ Modelo guardado en ./detr_monedas_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc17af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé§ Resumen para la Entrevista\n",
    "\n",
    "> *\"Desarroll√© un pipeline completo de visi√≥n artificial: captur√© y anot√© mi propio dataset en **CVAT** (formato COCO), realic√© fine-tuning de un modelo **DETR** usando Hugging Face Transformers para detectar objetos peque√±os con precisi√≥n, y finalmente export√© el modelo a **ONNX** para optimizar la inferencia en producci√≥n. El proyecto demuestra mi capacidad para manejar el ciclo completo de un sistema de ML, desde la anotaci√≥n de datos hasta el despliegue.\"*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
